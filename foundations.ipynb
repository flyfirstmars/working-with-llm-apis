{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b188803",
   "metadata": {},
   "source": [
    "# The Fundamentals of Working with LLM API\n",
    "\n",
    "A practical guide to working with Large Language Model APIs.\n",
    "\n",
    "**Workshop Goals:**\n",
    "- Understand how LLMs reason through prompts and take actions through tools\n",
    "- Analyze stateless API mechanics and token economics\n",
    "- Implement tool calling, streaming, and structured outputs\n",
    "- Apply context management strategies for production workflows\n",
    "\n",
    "**Prerequisites:** Python 3.8+, `requests` library, DIAL API/Azure OpenAI/OpenAI key\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction to LLM APIs\n",
    "\n",
    "### What are LLM APIs?\n",
    "\n",
    "Large Language Model APIs are **HTTP endpoints that transform natural language into structured outputs**.\n",
    "\n",
    "Think of it like this:\n",
    "```\n",
    "You send text -> Model processes -> You receive generated text\n",
    "```\n",
    "\n",
    "That's it! The API is **stateless** (no memory), works with **tokens** (text units), and responds based on how you **prompt** it.\n",
    "\n",
    "### Workshop Roadmap\n",
    "\n",
    "We'll cover:\n",
    "1. **Setup & First Call** - Get up and running\n",
    "2. **Tokens & Costs** - Understanding billing units\n",
    "3. **Prompts & Parameters** - Controlling model behavior (reasoning)\n",
    "4. **Conversations** - Managing multi-turn interactions\n",
    "5. **Tool Calling** - Enabling actions beyond text\n",
    "6. **Context Engineering** - Working within limits\n",
    "7. **Structured Outputs** - Getting JSON responses\n",
    "8. **Streaming** - Real-time token delivery\n",
    "9. **Vision** - Working with images\n",
    "10. **Production Patterns** - Error handling & best practices\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e769c273",
   "metadata": {},
   "source": [
    "## Part 1: Setup & First API Call\n",
    "\n",
    "### Theory: Chat Completions API\n",
    "\n",
    "**Endpoint Structure:**\n",
    "```\n",
    "https://{your-endpoint}/openai/deployments/{deployment-name}/chat/completions?api-version=2024-10-21\n",
    "```\n",
    "\n",
    "**Required Headers:**\n",
    "- `Content-Type: application/json`\n",
    "- `api-key: {your-api-key}`\n",
    "\n",
    "**Request Structure:**\n",
    "```json\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello!\"}\n",
    "  ],\n",
    "  \"max_completion_tokens\": 100\n",
    "}\n",
    "```\n",
    "\n",
    "**Response Structure:**\n",
    "```json\n",
    "{\n",
    "  \"choices\": [{\"message\": {\"content\": \"...\"}, \"finish_reason\": \"stop\"}],\n",
    "  \"usage\": {\"prompt_tokens\": 10, \"completion_tokens\": 20, \"total_tokens\": 30}\n",
    "}\n",
    "```\n",
    "\n",
    "**GPT-5 vs GPT-4o Parameters:**\n",
    "- **GPT-5 (reasoning):** `max_completion_tokens`, `reasoning_effort`, `developer` role\n",
    "- **GPT-4o (standard):** `max_tokens`, `temperature`, `system` role\n",
    "\n",
    "---\n",
    "\n",
    "### Demo 1.1: Configure API Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7749b91d9da04d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# Collect DIAL configuration\n",
    "if 'DIAL_API_KEY' not in os.environ or not os.environ['DIAL_API_KEY']:\n",
    "    os.environ['DIAL_API_KEY'] = getpass.getpass('Enter DIAL API Key: ')\n",
    "\n",
    "# Set defaults (no prompting - use environment variables or defaults)\n",
    "os.environ.setdefault('DIAL_DEPLOYMENT', 'gpt-5-mini-2025-08-07')\n",
    "os.environ.setdefault('DIAL_GPT4O_DEPLOYMENT', 'gpt-4o-mini-2024-07-18')\n",
    "os.environ.setdefault('DIAL_API_ENDPOINT', 'https://ai-proxy.lab.epam.com')\n",
    "os.environ.setdefault('DIAL_API_VERSION', '2024-10-21')\n",
    "\n",
    "# Remove any trailing slash to avoid double-slash URLs\n",
    "os.environ['DIAL_API_ENDPOINT'] = os.environ['DIAL_API_ENDPOINT'].rstrip('/')\n",
    "\n",
    "# Detect if this is a reasoning model\n",
    "deployment = os.environ['DIAL_DEPLOYMENT'].lower()\n",
    "reasoning_models = ['gpt-5', 'o1', 'o3', 'o4']\n",
    "is_reasoning_model = any(model in deployment for model in reasoning_models)\n",
    "\n",
    "print('[OK] Configuration set:')\n",
    "print(f\"  Endpoint: {os.environ['DIAL_API_ENDPOINT']}\")\n",
    "print(f\"  API Version: {os.environ['DIAL_API_VERSION']}\")\n",
    "print(f\"  API Key: {'*' * 20} (hidden)\")\n",
    "print(f\"\\n  Primary Model: {os.environ['DIAL_DEPLOYMENT']}\")\n",
    "print(f\"  Comparison Model: {os.environ['DIAL_GPT4O_DEPLOYMENT']}\")\n",
    "\n",
    "if is_reasoning_model:\n",
    "    print(f\"\\n[OK] GPT-5 Reasoning Model Detected:\")\n",
    "    print(\"  [OK] Supported: max_completion_tokens, reasoning_effort, developer role\")\n",
    "    print(\"  [X] NOT supported: temperature, top_p, max_tokens\")\n",
    "    os.environ['MODEL_TYPE'] = 'reasoning'\n",
    "else:\n",
    "    print(f\"\\n[OK] Standard Model Detected (GPT-4o):\")\n",
    "    print(\"  [OK] Supported: max_tokens, temperature, top_p, system role\")\n",
    "    os.environ['MODEL_TYPE'] = 'standard'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1081a15907f2b7",
   "metadata": {},
   "source": [
    "### Demo 1.2: First API Call - Hello World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73aabb0540482fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Build request\n",
    "endpoint = os.environ['DIAL_API_ENDPOINT']\n",
    "deployment = os.environ['DIAL_DEPLOYMENT']\n",
    "api_version = os.environ['DIAL_API_VERSION']\n",
    "api_key = os.environ['DIAL_API_KEY']\n",
    "\n",
    "url = f\"{endpoint}/openai/deployments/{deployment}/chat/completions?api-version={api_version}\"\n",
    "headers = {\"Content-Type\": \"application/json\", \"api-key\": api_key}\n",
    "\n",
    "payload = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"developer\", \"content\": \"You are a helpful assistant. Respond directly without extensive reasoning.\"},\n",
    "        {\"role\": \"user\", \"content\": \"what's 2 + 2.\"}\n",
    "    ],\n",
    "    \"max_completion_tokens\": 2000,  # GPT-5 uses tokens for reasoning + output\n",
    "    \"reasoning_effort\": \"low\"   # Use minimal reasoning for simple responses or low for none\n",
    "}\n",
    "\n",
    "# Call API\n",
    "response = requests.post(url, headers=headers, json=payload)\n",
    "result = response.json()\n",
    "\n",
    "# Display results\n",
    "content = result['choices'][0]['message'].get('content', '')\n",
    "print(\"Response:\", content if content else \"(empty - model used reasoning tokens only)\")\n",
    "print(f\"\\nToken usage:\")\n",
    "print(f\"  Prompt: {result['usage']['prompt_tokens']}\")\n",
    "print(f\"  Completion: {result['usage']['completion_tokens']}\")\n",
    "print(f\"  Total: {result['usage']['total_tokens']}\")\n",
    "\n",
    "# Check for reasoning tokens (GPT-5 specific)\n",
    "if 'completion_tokens_details' in result['usage']:\n",
    "    reasoning_tokens = result['usage']['completion_tokens_details'].get('reasoning_tokens', 0)\n",
    "    if reasoning_tokens > 0:\n",
    "        print(f\"  Reasoning: {reasoning_tokens}\")\n",
    "\n",
    "print(f\"  Finish reason: {result['choices'][0]['finish_reason']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feba770e2dc8a973",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Understanding Tokens & Costs\n",
    "\n",
    "### Theory: What are Tokens?\n",
    "\n",
    "**Tokens** are the fundamental units that LLMs process:\n",
    "- Subword pieces: can be as short as 1 character or as long as 1 word\n",
    "- **Context sensitivity:** \" red\", \" Red\", \"Red\" -> 3 different tokens!\n",
    "- **Rough estimation:** ~1 token = 4 characters or 0.75 words\n",
    "- **For exact counts:** Use tokenizers or check API response `usage` field\n",
    "\n",
    "### Four Token Types\n",
    "\n",
    "1. **Input tokens:** Your request (prompt, instructions, conversation history)\n",
    "2. **Output tokens:** Generated response\n",
    "3. **Cached tokens:** Reused context (billed at reduced rates)\n",
    "4. **Reasoning tokens:** Internal \"thinking\" (GPT-5, o-series only)\n",
    "\n",
    "### Pricing Model\n",
    "\n",
    "**Output tokens cost 2-3x more than input tokens!**\n",
    "\n",
    "**2025 Pricing Reference** (per 1M tokens):\n",
    "\n",
    "| Model | Input | Output |\n",
    "|-------|-------|--------|\n",
    "| GPT-5 | $1.25 | $10.00 |\n",
    "| GPT-5-mini | $0.25 | $2.00 |\n",
    "| GPT-4o | $2.50 | $10.00 |\n",
    "| GPT-4o-mini | $0.15 | $0.60 |\n",
    "\n",
    "### Usage Field Breakdown\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"usage\": {\n",
    "    \"prompt_tokens\": 245,\n",
    "    \"completion_tokens\": 892,\n",
    "    \"total_tokens\": 1137,\n",
    "    \"completion_tokens_details\": {\n",
    "      \"reasoning_tokens\": 156\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Demo 2.1: Token Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d25e75c562acae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple demo showing token counting\n",
    "text = \"Explain REST APIs in one sentence.\"\n",
    "\n",
    "# Build request\n",
    "url = f\"{os.environ['DIAL_API_ENDPOINT']}/openai/deployments/{os.environ['DIAL_DEPLOYMENT']}/chat/completions?api-version={os.environ['DIAL_API_VERSION']}\"\n",
    "headers = {\"Content-Type\": \"application/json\", \"api-key\": os.environ['DIAL_API_KEY']}\n",
    "payload = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"developer\", \"content\": \"Respond directly without reasoning.\"},\n",
    "        {\"role\": \"user\", \"content\": text}\n",
    "    ],\n",
    "    \"max_completion_tokens\": 500,\n",
    "    \"reasoning_effort\": \"minimal\"  # Minimal reasoning for simple responses\n",
    "}\n",
    "\n",
    "# Call API\n",
    "response = requests.post(url, headers=headers, json=payload)\n",
    "result = response.json()\n",
    "\n",
    "# Extract content and check for reasoning tokens\n",
    "content = result['choices'][0]['message'].get('content', '')\n",
    "reasoning_tokens = result['usage'].get('completion_tokens_details', {}).get('reasoning_tokens', 0)\n",
    "\n",
    "# Display token breakdown\n",
    "print(f\"Input text: '{text}'\")\n",
    "print(f\"\\nToken usage:\")\n",
    "print(f\"  Prompt tokens: {result['usage']['prompt_tokens']}\")\n",
    "print(f\"  Completion tokens: {result['usage']['completion_tokens']}\")\n",
    "if reasoning_tokens > 0:\n",
    "    print(f\"    └─ Reasoning tokens: {reasoning_tokens}\")\n",
    "    print(f\"    └─ Output tokens: {result['usage']['completion_tokens'] - reasoning_tokens}\")\n",
    "print(f\"  Total tokens: {result['usage']['total_tokens']}\")\n",
    "\n",
    "# Estimate cost (GPT-5-mini rates: $0.25 input, $2.00 output per 1M tokens)\n",
    "input_cost = result['usage']['prompt_tokens'] * 0.00000025\n",
    "output_cost = result['usage']['completion_tokens'] * 0.000002\n",
    "total_cost = input_cost + output_cost\n",
    "print(f\"\\nEstimated cost: ${total_cost:.6f}\")\n",
    "print(f\"  └─ Input: ${input_cost:.6f} ({result['usage']['prompt_tokens']} × $0.25/1M)\")\n",
    "print(f\"  └─ Output: ${output_cost:.6f} ({result['usage']['completion_tokens']} × $2.00/1M)\")\n",
    "\n",
    "print(f\"\\nResponse: {content if content else '(empty - all tokens used for reasoning)'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cd30113ca3da2f",
   "metadata": {},
   "source": [
    "### Demo 2.2: Comparing Reasoning Efforts (GPT-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11eb4d248bf0c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare low vs high reasoning effort\n",
    "question = \"Calculate 15 items/hour × 8 hours/day × 7 days, minus 20% returns.\"\n",
    "\n",
    "# Low reasoning effort\n",
    "payload_low = {\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": question}],\n",
    "    \"max_completion_tokens\": 2000,\n",
    "    \"reasoning_effort\": \"low\"\n",
    "}\n",
    "response_low = requests.post(url, headers=headers, json=payload_low)\n",
    "result_low = response_low.json()\n",
    "\n",
    "# High reasoning effort\n",
    "payload_high = {\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": question}],\n",
    "    \"max_completion_tokens\": 2000,  # Same limit for fair comparison\n",
    "    \"reasoning_effort\": \"high\"\n",
    "}\n",
    "response_high = requests.post(url, headers=headers, json=payload_high)\n",
    "result_high = response_high.json()\n",
    "\n",
    "# Calculate costs (GPT-5-mini: $0.25 input, $2.00 output per 1M)\n",
    "def calculate_cost(usage):\n",
    "    input_cost = usage['prompt_tokens'] * 0.00000025\n",
    "    output_cost = usage['completion_tokens'] * 0.000002\n",
    "    return input_cost + output_cost\n",
    "\n",
    "# Compare with actual responses\n",
    "print(\"=\" * 70)\n",
    "print(\"LOW reasoning effort:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"Answer: {result_low['choices'][0]['message'].get('content', '(empty)')}\")\n",
    "print(f\"\\nTokens:\")\n",
    "print(f\"  Reasoning: {result_low['usage'].get('completion_tokens_details', {}).get('reasoning_tokens', 0)}\")\n",
    "print(f\"  Total: {result_low['usage']['total_tokens']}\")\n",
    "print(f\"  Cost: ${calculate_cost(result_low['usage']):.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"HIGH reasoning effort:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"Answer: {result_high['choices'][0]['message'].get('content', '(empty)')}\")\n",
    "print(f\"\\nTokens:\")\n",
    "print(f\"  Reasoning: {result_high['usage'].get('completion_tokens_details', {}).get('reasoning_tokens', 0)}\")\n",
    "print(f\"  Total: {result_high['usage']['total_tokens']}\")\n",
    "print(f\"  Cost: ${calculate_cost(result_high['usage']):.6f}\")\n",
    "\n",
    "# Show comparison\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "reasoning_diff = result_high['usage'].get('completion_tokens_details', {}).get('reasoning_tokens', 0) - result_low['usage'].get('completion_tokens_details', {}).get('reasoning_tokens', 0)\n",
    "cost_diff = calculate_cost(result_high['usage']) - calculate_cost(result_low['usage'])\n",
    "print(f\"HIGH reasoning used {reasoning_diff} more reasoning tokens\")\n",
    "print(f\"Cost difference: ${abs(cost_diff):.6f} ({'higher' if cost_diff > 0 else 'lower'} with high reasoning)\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6846a0b6117a17",
   "metadata": {},
   "source": [
    "### Demo 2.3: Truncation - finish_reason: \"length\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49090077332a73c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate truncation with small token limit\n",
    "payload = {\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Explain REST APIs in detail covering requests, verbs, codes, best practices.\"}],\n",
    "    \"max_completion_tokens\": 30,  # Too small!\n",
    "    \"reasoning_effort\": \"low\"\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=payload)\n",
    "result = response.json()\n",
    "\n",
    "print(f\"Response: {result['choices'][0]['message']['content']}\")\n",
    "print(f\"\\nFinish reason: {result['choices'][0]['finish_reason']}\")\n",
    "\n",
    "if result['choices'][0]['finish_reason'] == 'length':\n",
    "    print(\"  TRUNCATED! Response was cut off. Increase max_completion_tokens.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b7af00",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Prompts & Model Parameters (REASONING)\n",
    "\n",
    "LLMs reason through the prompts you craft and parameters you set.\n",
    "\n",
    "### Theory: Prompt Engineering Fundamentals\n",
    "\n",
    "**Clear Context & Instructions:**\n",
    "- Provide role, context, and explicit constraints\n",
    "- Example: \"You are a financial analyst. Explain in 3 sentences how to evaluate a company's balance sheet.\"\n",
    "\n",
    "**Few-Shot Prompting:**\n",
    "- Provide 2-5 examples to teach the pattern\n",
    "- Example: Show sentiment examples (positive/negative) then ask model to classify new text\n",
    "\n",
    "**Chain-of-Thought (CoT):**\n",
    "- Ask model to reason step-by-step before answering\n",
    "- Add \"Let's think step by step\" or \"Show your work\"\n",
    "- Improves accuracy on complex reasoning tasks\n",
    "- Not needed for Reasoning/Thinking models\n",
    "\n",
    "**Constraint-Based:**\n",
    "- Specify output length, format, what to avoid\n",
    "- Example: \"Summarize in exactly 100 words without using the word 'technology'\"\n",
    "\n",
    "**Advanced (for reference):** Reflexion, Tree-of-Thoughts, Analogical Prompting\n",
    "### Theory: Model Parameters\n",
    "\n",
    "**GPT-5 (Reasoning Model):**\n",
    "- `max_completion_tokens`: output limit\n",
    "- `reasoning_effort`: minimal/low/medium/high (controls thinking depth, replaces temperature)\n",
    "- `developer` role: system instructions for reasoning models\n",
    "- No temperature/top_p (use `reasoning_effort` to control output variability)\n",
    "\n",
    "**GPT-4o (Standard Model):**\n",
    "- `max_tokens`: output limit\n",
    "- `temperature` (0.0-2.0): deterministic (low) vs creative (high)\n",
    "- `top_p`: nucleus sampling (use temp OR top_p, not both)\n",
    "- `system` role: persistent behavior instructions\n",
    "\n",
    "**When to use:**\n",
    "- GPT-5 for complex reasoning, coding, debugging\n",
    "- GPT-4o for speed, vision, creative tasks\n",
    "\n",
    "---\n",
    "\n",
    "### Demo 3.1: Few-Shot Prompting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff272f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few-shot: sentiment classification with examples\n",
    "payload = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"developer\", \"content\": \"Analyze product review sentiment.\"},\n",
    "        # Example 1\n",
    "        {\"role\": \"user\", \"content\": \"Review: The headphones broke after one week.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"SENTIMENT: Negative\"},\n",
    "        # Example 2\n",
    "        {\"role\": \"user\", \"content\": \"Review: Decent product for the price.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"SENTIMENT: Neutral\"},\n",
    "        # Example 3\n",
    "        {\"role\": \"user\", \"content\": \"Review: Absolutely love it! Best purchase ever.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"SENTIMENT: Positive\"},\n",
    "        # Actual query\n",
    "        {\"role\": \"user\", \"content\": \"Review: This laptop exceeded my expectations! Fast and reliable.\"}\n",
    "    ],\n",
    "    \"max_completion_tokens\": 1500,\n",
    "    \"reasoning_effort\": \"low\"\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=payload)\n",
    "result = response.json()\n",
    "print(\"Few-shot classification:\")\n",
    "print(result['choices'][0]['message']['content'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbda70a8",
   "metadata": {},
   "source": [
    "### Demo 3.2: Chain-of-Thought Reasoning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5823088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain-of-Thought: with vs without \"step by step\"\n",
    "problem = \"If a train travels 60 km/h for 2.5 hours, how far does it go?\"\n",
    "\n",
    "# Without CoT\n",
    "payload_no_cot = {\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": problem}],\n",
    "    \"max_completion_tokens\": 2000,\n",
    "    \"reasoning_effort\": \"minimal\"\n",
    "}\n",
    "response_no_cot = requests.post(url, headers=headers, json=payload_no_cot)\n",
    "result_no_cot = response_no_cot.json()\n",
    "\n",
    "# With CoT\n",
    "payload_cot = {\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": f\"{problem} Let's think step by step.\"}],\n",
    "    \"max_completion_tokens\": 3000,\n",
    "    \"reasoning_effort\": \"minimal\"\n",
    "}\n",
    "response_cot = requests.post(url, headers=headers, json=payload_cot)\n",
    "result_cot = response_cot.json()\n",
    "\n",
    "print(\"WITHOUT Chain-of-Thought:\")\n",
    "print(result_no_cot['choices'][0]['message']['content'])\n",
    "\n",
    "print(\"\\nWITH Chain-of-Thought:\")\n",
    "print(result_cot['choices'][0]['message']['content'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee42a781",
   "metadata": {},
   "source": [
    "### Demo 3.3: Temperature Control (GPT-4o)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08da2a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature comparison using GPT-4o\n",
    "gpt4o_url = f\"{os.environ['DIAL_API_ENDPOINT']}/openai/deployments/{os.environ['DIAL_GPT4O_DEPLOYMENT']}/chat/completions?api-version={os.environ['DIAL_API_VERSION']}\"\n",
    "\n",
    "prompt = \"Write a creative company name for a coffee shop.\"\n",
    "\n",
    "# Low temperature (deterministic)\n",
    "payload_low = {\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "    \"max_tokens\": 50,\n",
    "    \"temperature\": 0.0\n",
    "}\n",
    "response_low = requests.post(gpt4o_url, headers=headers, json=payload_low)\n",
    "result_low = response_low.json()\n",
    "\n",
    "# High temperature (creative)\n",
    "payload_high = {\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "    \"max_tokens\": 50,\n",
    "    \"temperature\": 1.5\n",
    "}\n",
    "response_high = requests.post(gpt4o_url, headers=headers, json=payload_high)\n",
    "result_high = response_high.json()\n",
    "\n",
    "print(f\"Temperature 0.0 (deterministic): {result_low['choices'][0]['message']['content']}\")\n",
    "print(f\"Temperature 1.5 (creative): {result_high['choices'][0]['message']['content']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fe9a6f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Multi-Turn Conversations & Stateless APIs\n",
    "\n",
    "### Theory: APIs Have NO Memory\n",
    "\n",
    "APIs are stateless - each request is independent.\n",
    "\n",
    "- No memory between requests: Server retains nothing\n",
    "- You manage history: Maintain messages array, append each response\n",
    "- Message pattern: system -> user -> assistant -> user -> assistant\n",
    "\n",
    "### Rate Limits\n",
    "\n",
    "- **RPM (Requests Per Minute)** and **TPM (Tokens Per Minute)**\n",
    "- **HTTP 429 error** -> exponential backoff required\n",
    "- Retry pattern: wait 1s, 2s, 4s, 8s...\n",
    "\n",
    "---\n",
    "\n",
    "### Demo 4.1: 3-Turn Conversation with Manual History\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae1b1cf53fe714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-turn conversation with manual history management\n",
    "messages = [\n",
    "    {\"role\": \"developer\", \"content\": \"You are a DevOps expert.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is Docker?\"}\n",
    "]\n",
    "\n",
    "# Turn 1\n",
    "response1 = requests.post(url, headers=headers, json={\n",
    "    \"messages\": messages,\n",
    "    \"max_completion_tokens\": 100,\n",
    "    \"reasoning_effort\": \"low\"\n",
    "})\n",
    "result1 = response1.json()\n",
    "reply1 = result1['choices'][0]['message']['content']\n",
    "\n",
    "print(f\"Turn 1 - Total tokens: {result1['usage']['total_tokens']}\")\n",
    "print(f\"Messages in history: {len(messages)}\\n\")\n",
    "\n",
    "# Add assistant response to history\n",
    "messages.append({\"role\": \"assistant\", \"content\": reply1})\n",
    "messages.append({\"role\": \"user\", \"content\": \"How does it differ from a VM?\"})\n",
    "\n",
    "# Turn 2\n",
    "response2 = requests.post(url, headers=headers, json={\n",
    "    \"messages\": messages,\n",
    "    \"max_completion_tokens\": 100,\n",
    "    \"reasoning_effort\": \"low\"\n",
    "})\n",
    "result2 = response2.json()\n",
    "reply2 = result2['choices'][0]['message']['content']\n",
    "\n",
    "print(f\"Turn 2 - Total tokens: {result2['usage']['total_tokens']}\")\n",
    "print(f\"Messages in history: {len(messages)}\\n\")\n",
    "\n",
    "# Add to history again\n",
    "messages.append({\"role\": \"assistant\", \"content\": reply2})\n",
    "messages.append({\"role\": \"user\", \"content\": \"Give a minimal docker-compose example.\"})\n",
    "\n",
    "# Turn 3\n",
    "response3 = requests.post(url, headers=headers, json={\n",
    "    \"messages\": messages,\n",
    "    \"max_completion_tokens\": 150,\n",
    "    \"reasoning_effort\": \"low\"\n",
    "})\n",
    "result3 = response3.json()\n",
    "\n",
    "print(f\"Turn 3 - Total tokens: {result3['usage']['total_tokens']}\")\n",
    "print(f\"Messages in history: {len(messages)}\")\n",
    "print(f\"\\nToken accumulation: {result1['usage']['total_tokens']} -> {result2['usage']['total_tokens']} -> {result3['usage']['total_tokens']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4b789ff4183d99",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Tool Calling & Function Calling (ACTIONS)\n",
    "\n",
    "LLMs take actions through structured function calls that your code executes.\n",
    "\n",
    "### Theory: Function Calling Workflow\n",
    "\n",
    "LLMs generate JSON for tool calls, your code executes them.\n",
    "\n",
    "**5-Step Process:**\n",
    "1. Send request with tool definitions (JSON Schema in `tools` parameter)\n",
    "2. Model decides to call (`finish_reason: \"tool_calls\"`)\n",
    "3. **Your Python code** executes the actual function\n",
    "4. Return results with `role: \"tool\"`\n",
    "5. Model synthesizes final natural language answer\n",
    "\n",
    "**Control:** `tool_choice` = `\"auto\"` (default), `\"none\"`, `\"required\"`, or specific function name\n",
    "\n",
    "**Best Practice:** Design tools like clean APIs - single responsibility, clear names, use enums for constrained values.\n",
    "\n",
    "---\n",
    "\n",
    "### Demo 5.1: Single Tool (Calculator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7b6dbfca374254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple calculator tool\n",
    "def calculate(operation, x, y):\n",
    "    \"\"\"Execute a mathematical operation.\"\"\"\n",
    "    operations = {\n",
    "        'add': lambda a, b: a + b,\n",
    "        'subtract': lambda a, b: a - b,\n",
    "        'multiply': lambda a, b: a * b,\n",
    "        'divide': lambda a, b: a / b if b != 0 else 'Error: Division by zero'\n",
    "    }\n",
    "    return operations.get(operation, lambda a, b: 'Unknown operation')(x, y)\n",
    "\n",
    "# Define the tool schema for the model\n",
    "calculator_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"calculate\",\n",
    "        \"description\": \"Perform basic arithmetic operations (add, subtract, multiply, divide)\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"operation\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"add\", \"subtract\", \"multiply\", \"divide\"],\n",
    "                    \"description\": \"The arithmetic operation to perform\"\n",
    "                },\n",
    "                \"x\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"description\": \"The first number\"\n",
    "                },\n",
    "                \"y\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"description\": \"The second number\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"operation\", \"x\", \"y\"],\n",
    "            \"additionalProperties\": False\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initial request with tool definition\n",
    "tool_payload = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"developer\",\n",
    "            \"content\": \"You are a helpful math assistant. Use the calculator tool when needed.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is 127 multiplied by 89?\"\n",
    "        }\n",
    "    ],\n",
    "    \"tools\": [calculator_tool],\n",
    "    \"tool_choice\": \"auto\",\n",
    "    \"max_completion_tokens\": 2000,\n",
    "    \"reasoning_effort\": \"low\"\n",
    "}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STEP 1: Sending request with tool definition\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Send request\n",
    "response1 = requests.post(url, headers=headers, json=tool_payload)\n",
    "result1 = response1.json()\n",
    "choice1 = result1['choices'][0]\n",
    "message1 = choice1['message']\n",
    "\n",
    "print(f\"\\nFinish reason: {choice1['finish_reason']}\")\n",
    "print(f\"Model response: {message1.get('content', '(no text content)')}\")\n",
    "\n",
    "# Check if model wants to call a tool\n",
    "if choice1['finish_reason'] == 'tool_calls' and 'tool_calls' in message1:\n",
    "    tool_call = message1['tool_calls'][0]\n",
    "    function_name = tool_call['function']['name']\n",
    "    function_args = json.loads(tool_call['function']['arguments'])\n",
    "    \n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(\"STEP 2: Model requested tool call\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "    print(f\"Function: {function_name}\")\n",
    "    print(f\"Arguments: {json.dumps(function_args, indent=2)}\")\n",
    "    \n",
    "    # Execute the function\n",
    "    result = calculate(**function_args)\n",
    "    print(f\"\\nFunction result: {result}\")\n",
    "    \n",
    "    # Append the assistant's tool call request\n",
    "    tool_payload['messages'].append(message1)\n",
    "    \n",
    "    # Append the tool result\n",
    "    tool_payload['messages'].append({\n",
    "        \"role\": \"tool\",\n",
    "        \"tool_call_id\": tool_call['id'],\n",
    "        \"content\": json.dumps({\"result\": result})\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(\"STEP 3: Sending tool result back to model\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "    \n",
    "    # Send back with tool result\n",
    "    response2 = requests.post(url, headers=headers, json=tool_payload)\n",
    "    result2 = response2.json()\n",
    "    final_response = result2['choices'][0]['message']['content']\n",
    "    \n",
    "    print(f\"\\nFinal response:\")\n",
    "    print(final_response)\n",
    "    print(f\"\\nFinish reason: {result2['choices'][0]['finish_reason']}\")\n",
    "else:\n",
    "    print(\"\\nModel responded without calling tool (unexpected for this example)\")\n",
    "\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(\"Tool calling demo complete!\")\n",
    "print(f\"{'=' * 70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0700536c9baabf",
   "metadata": {},
   "source": [
    "### Demo 5.2: Multiple Tools\n",
    "\n",
    "Model chooses between tools, can call in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f916ff95c4206339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define multiple tools\n",
    "def get_current_weather(location, units=\"celsius\"):\n",
    "    \"\"\"Simulated weather API.\"\"\"\n",
    "    # In real scenario, this would call an actual API\n",
    "    weather_data = {\n",
    "        \"San Francisco\": {\"temp\": 18, \"conditions\": \"Partly cloudy\"},\n",
    "        \"Tokyo\": {\"temp\": 22, \"conditions\": \"Sunny\"},\n",
    "        \"London\": {\"temp\": 12, \"conditions\": \"Rainy\"},\n",
    "        \"Paris\": {\"temp\": 15, \"conditions\": \"Overcast\"}\n",
    "    }\n",
    "    data = weather_data.get(location, {\"temp\": 20, \"conditions\": \"Unknown\"})\n",
    "    return {\n",
    "        \"location\": location,\n",
    "        \"temperature\": data[\"temp\"],\n",
    "        \"units\": units,\n",
    "        \"conditions\": data[\"conditions\"]\n",
    "    }\n",
    "\n",
    "def search_database(query, limit=5):\n",
    "    \"\"\"Simulated database search.\"\"\"\n",
    "    # Simulate a product database\n",
    "    products = [\n",
    "        {\"id\": 1, \"name\": \"Laptop Pro 15\", \"price\": 1299, \"category\": \"Electronics\"},\n",
    "        {\"id\": 2, \"name\": \"Wireless Mouse\", \"price\": 29, \"category\": \"Electronics\"},\n",
    "        {\"id\": 3, \"name\": \"Office Chair\", \"price\": 249, \"category\": \"Furniture\"},\n",
    "        {\"id\": 4, \"name\": \"Desk Lamp\", \"price\": 45, \"category\": \"Furniture\"},\n",
    "        {\"id\": 5, \"name\": \"Notebook Set\", \"price\": 12, \"category\": \"Stationery\"}\n",
    "    ]\n",
    "    # Simple search simulation\n",
    "    results = [p for p in products if query.lower() in p['name'].lower() or query.lower() in p['category'].lower()]\n",
    "    return results[:limit]\n",
    "\n",
    "# Define tool schemas\n",
    "weather_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_current_weather\",\n",
    "        \"description\": \"Get the current weather in a given location\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The city name, e.g., San Francisco, Tokyo\"\n",
    "                },\n",
    "                \"units\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                    \"description\": \"Temperature units\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"location\"],\n",
    "            \"additionalProperties\": False\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "database_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"search_database\",\n",
    "        \"description\": \"Search the product database for items matching a query\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Search query string\"\n",
    "                },\n",
    "                \"limit\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"Maximum number of results to return\",\n",
    "                    \"default\": 5\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"],\n",
    "            \"additionalProperties\": False\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Function dispatcher\n",
    "tool_functions = {\n",
    "    \"get_current_weather\": get_current_weather,\n",
    "    \"search_database\": search_database\n",
    "}\n",
    "\n",
    "# Multi-tool request\n",
    "multi_tool_payload = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"developer\",\n",
    "            \"content\": \"You are a helpful assistant with access to weather data and a product database.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What's the weather in Tokyo and can you find electronics in the database?\"\n",
    "        }\n",
    "    ],\n",
    "    \"tools\": [weather_tool, database_tool],\n",
    "    \"tool_choice\": \"auto\",\n",
    "    \"max_completion_tokens\": 3000,\n",
    "    \"reasoning_effort\": \"low\"\n",
    "}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Multi-Tool Demo: Weather + Database\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# First request\n",
    "response1 = requests.post(url, headers=headers, json=multi_tool_payload)\n",
    "result1 = response1.json()\n",
    "choice1 = result1['choices'][0]\n",
    "message1 = choice1['message']\n",
    "\n",
    "print(f\"\\nModel's initial response:\")\n",
    "print(f\"Finish reason: {choice1['finish_reason']}\")\n",
    "\n",
    "if 'tool_calls' in message1:\n",
    "    print(f\"\\nModel requested {len(message1['tool_calls'])} tool call(s):\")\n",
    "    \n",
    "    # Append assistant message\n",
    "    multi_tool_payload['messages'].append(message1)\n",
    "    \n",
    "    # Execute all tool calls\n",
    "    for tool_call in message1['tool_calls']:\n",
    "        function_name = tool_call['function']['name']\n",
    "        function_args = json.loads(tool_call['function']['arguments'])\n",
    "        \n",
    "        print(f\"\\n  Tool: {function_name}\")\n",
    "        print(f\"  Args: {json.dumps(function_args, indent=4)}\")\n",
    "        \n",
    "        # Execute function\n",
    "        function_result = tool_functions[function_name](**function_args)\n",
    "        print(f\"  Result: {json.dumps(function_result, indent=4)}\")\n",
    "        \n",
    "        # Append tool result\n",
    "        multi_tool_payload['messages'].append({\n",
    "            \"role\": \"tool\",\n",
    "            \"tool_call_id\": tool_call['id'],\n",
    "            \"content\": json.dumps(function_result)\n",
    "        })\n",
    "    \n",
    "    # Send results back to model\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(\"Sending tool results back to model...\")\n",
    "    print(f\"{'=' * 70}\\n\")\n",
    "    \n",
    "    response2 = requests.post(url, headers=headers, json=multi_tool_payload)\n",
    "    result2 = response2.json()\n",
    "    final_message = result2['choices'][0]['message']['content']\n",
    "    \n",
    "    print(\"Final response:\")\n",
    "    print(final_message)\n",
    "else:\n",
    "    print(\"\\nNo tool calls requested\")\n",
    "    print(message1.get('content', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a87183",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Context Engineering & Limitations\n",
    "\n",
    "### Theory: Finite Context & Quality Degradation\n",
    "\n",
    "**Context Window Limits:**\n",
    "- GPT-4o/GPT-5: 128K tokens standard\n",
    "- GPT-5 can extend to 272K (preview)\n",
    "- Quality drops beyond 50-55% capacity\n",
    "\n",
    "**Context Rot (Lost-in-the-Middle):**\n",
    "- Attention complexity: O(n²) - quadratic growth\n",
    "- Model misses details buried in long contexts\n",
    "- Keep high-signal content at top and bottom\n",
    "- Find the \"smallest possible set of high-signal tokens\"\n",
    "\n",
    "### Organization Best Practices\n",
    "\n",
    "**Prompt Structure:**\n",
    "1. System/developer instructions (top)\n",
    "2. Tool definitions\n",
    "3. Conversation history\n",
    "4. Current user query (bottom)\n",
    "\n",
    "**Use Structure Tags:**\n",
    "- XML: `<instructions>`, `<context>`, `<examples>`\n",
    "- Markdown: Clear headers and sections\n",
    "- Specific enough to guide, flexible enough to generalize\n",
    "\n",
    "### Compaction Strategies\n",
    "\n",
    "**1. Summarization:** Compress old turns using the model itself\n",
    "- Combine system message + summary + recent turns\n",
    "- Maintains context while reducing tokens\n",
    "\n",
    "**2. Sliding Window:** Keep last N turns verbatim\n",
    "- Simple but loses earlier context\n",
    "- Good for chat interfaces\n",
    "\n",
    "**3. Progressive Disclosure:** Store IDs, load full content just-in-time\n",
    "- Significantly reduces token usage\n",
    "- Pattern: `listItems()` -> `getItemSummary(id)` -> `getItemContent(id)` only when needed\n",
    "\n",
    "**4. External Memory:** Note-taking or sub-agent patterns\n",
    "- Maintain structured memory files\n",
    "- Retrieve relevant sections via tools\n",
    "\n",
    "---\n",
    "\n",
    "### Demo 6.1: Conversation Summarization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4016ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context compaction demo\n",
    "def summarize_conversation(messages):\n",
    "    '''Compress old conversation turns into a summary.'''\n",
    "    summary_payload = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"developer\", \"content\": \"Summarize the following conversation concisely, preserving key information.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Conversation:\\n\" + \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in messages])}\n",
    "        ],\n",
    "        \"max_completion_tokens\": 2000,\n",
    "        \"reasoning_effort\": \"low\"\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, json=summary_payload)\n",
    "    result = response.json()\n",
    "    return result['choices'][0]['message']['content']\n",
    "\n",
    "# Simulate long conversation\n",
    "conversation = [\n",
    "    {\"role\": \"developer\", \"content\": \"You are a helpful coding assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is Python?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Python is a high-level, interpreted programming language known for its simplicity and readability.\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I create a list?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Use square brackets: my_list = [1, 2, 3] or list() constructor.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What about dictionaries?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Dictionaries use curly braces: my_dict = {'key': 'value'}\"}\n",
    "]\n",
    "\n",
    "print(\"Original conversation: {} messages, ~{} chars\".format(len(conversation), sum(len(str(m)) for m in conversation)))\n",
    "\n",
    "# Summarize old turns (keep system + last 2)\n",
    "old_turns = conversation[1:-2]\n",
    "summary = summarize_conversation(old_turns)\n",
    "\n",
    "print(f\"\\nSummary of old turns:\\n{summary}\")\n",
    "\n",
    "# Rebuild with summary\n",
    "compacted = [\n",
    "    conversation[0],  # system\n",
    "    {\"role\": \"user\", \"content\": f\"[Previous: {summary}]\"},\n",
    "    conversation[-2],  # recent messages\n",
    "    conversation[-1]\n",
    "]\n",
    "\n",
    "print(f\"\\nCompacted conversation: {len(compacted)} messages, ~{sum(len(str(m)) for m in compacted)} chars\")\n",
    "print(f\"Token savings: ~{(1 - sum(len(str(m)) for m in compacted) / sum(len(str(m)) for m in conversation)) * 100:.0f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a37a830",
   "metadata": {},
   "source": [
    "### Demo 6.2: Progressive Disclosure with Tools\n",
    "\n",
    "Store lightweight IDs, load full content just-in-time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fc7834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a document database\n",
    "documents = {\n",
    "    \"doc1\": {\"title\": \"API Design Best Practices\", \"summary\": \"REST principles, versioning, error handling\", \"content\": \"... 5000 words ...\"},\n",
    "    \"doc2\": {\"title\": \"Database Optimization Guide\", \"summary\": \"Indexing, query optimization, caching strategies\", \"content\": \"... 8000 words ...\"},\n",
    "    \"doc3\": {\"title\": \"Kubernetes Deployment Patterns\", \"summary\": \"Rolling updates, blue-green, canary deployments\", \"content\": \"... 6000 words ...\"}\n",
    "}\n",
    "\n",
    "# Tool 1: List available documents\n",
    "def list_documents():\n",
    "    return [{\"id\": doc_id, \"title\": doc[\"title\"]} for doc_id, doc in documents.items()]\n",
    "\n",
    "# Tool 2: Get summary\n",
    "def get_document_summary(doc_id):\n",
    "    if doc_id in documents:\n",
    "        return {\"id\": doc_id, \"title\": documents[doc_id][\"title\"], \"summary\": documents[doc_id][\"summary\"]}\n",
    "    return {\"error\": \"Document not found\"}\n",
    "\n",
    "# Tool 3: Get full content (only when needed)\n",
    "def get_document_content(doc_id):\n",
    "    if doc_id in documents:\n",
    "        return {\"id\": doc_id, \"content\": documents[doc_id][\"content\"]}\n",
    "    return {\"error\": \"Document not found\"}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Progressive Disclosure Pattern\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nStep 1: User asks question\")\n",
    "print(\"Query: 'How do I implement blue-green deployments?'\")\n",
    "\n",
    "print(\"\\nStep 2: Model calls list_documents()\")\n",
    "doc_list = list_documents()\n",
    "print(f\"Available: {[d['title'] for d in doc_list]}\")\n",
    "\n",
    "print(\"\\nStep 3: Model identifies relevant doc, calls get_document_summary('doc3')\")\n",
    "summary = get_document_summary(\"doc3\")\n",
    "print(f\"Summary: {summary['summary']}\")\n",
    "\n",
    "print(\"\\nStep 4: Model determines it needs full content, calls get_document_content('doc3')\")\n",
    "print(\"(Only NOW do we load the heavy 6000-word document into context)\")\n",
    "\n",
    "print(\"\\nToken savings: Only loaded 1 document instead of all 3!\")\n",
    "print(\"Kept context focused and reduced cost by ~70%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91313b222154b315",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Structured Outputs & Data Extraction\n",
    "\n",
    "### Theory: JSON Mode vs Schemas\n",
    "\n",
    "**JSON Mode:** `response_format: {\"type\": \"json_object\"}`\n",
    "- Guarantees valid JSON syntax\n",
    "- Field names/types can vary\n",
    "\n",
    "**Strict Schemas:** `{\"type\": \"json_schema\", \"strict\": true}`\n",
    "- Enforces exact fields and types\n",
    "- All fields required, `additionalProperties: false`\n",
    "- First call ~10 sec (caching), then fast\n",
    "- Guarantees format, NOT accuracy\n",
    "\n",
    "---\n",
    "\n",
    "### Demo 7.1: JSON Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece1289475f8e052",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_payload = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"developer\",\n",
    "            \"content\": \"You extract entities and always respond with JSON containing name, age, occupation, city.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Jisoo Park is a 28-year-old Chief Meme Engineer living in Seoul, South Korea.\"\n",
    "        }\n",
    "    ],\n",
    "    \"max_completion_tokens\": 2000,\n",
    "    \"reasoning_effort\": \"low\",\n",
    "    \"response_format\": {\"type\": \"json_object\"}\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=json_payload)\n",
    "json_result = response.json()\n",
    "raw_content = json_result['choices'][0]['message']['content']\n",
    "\n",
    "print('Raw JSON response:\\n' + raw_content)\n",
    "\n",
    "try:\n",
    "    parsed = json.loads(raw_content)\n",
    "    print('\\nParsed entity:')\n",
    "    for key, val in parsed.items():\n",
    "        print(f'  {key}: {val}')\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f'\\n Failed to parse JSON: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc5079c384bb4eb",
   "metadata": {},
   "source": [
    "### Demo 7.2: Strict JSON Schema\n",
    "\n",
    "Enforce exact structure for user profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e9b05cff0d8028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a strict JSON schema for extracting user information\n",
    "user_schema = {\n",
    "    \"type\": \"json_schema\",\n",
    "    \"json_schema\": {\n",
    "        \"name\": \"user_profile_extraction\",\n",
    "        \"strict\": True,\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"personal_info\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"full_name\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The person's full name\"\n",
    "                        },\n",
    "                        \"age\": {\n",
    "                            \"type\": \"integer\",\n",
    "                            \"description\": \"The person's age in years\"\n",
    "                        },\n",
    "                        \"email\": {\n",
    "                            \"type\": [\"string\", \"null\"],\n",
    "                            \"description\": \"Email address if mentioned, otherwise null\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"full_name\", \"age\", \"email\"],\n",
    "                    \"additionalProperties\": False\n",
    "                },\n",
    "                \"professional_info\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"occupation\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Current job title or occupation\"\n",
    "                        },\n",
    "                        \"company\": {\n",
    "                            \"type\": [\"string\", \"null\"],\n",
    "                            \"description\": \"Company name if mentioned\"\n",
    "                        },\n",
    "                        \"years_of_experience\": {\n",
    "                            \"type\": [\"integer\", \"null\"],\n",
    "                            \"description\": \"Years of professional experience if mentioned\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"occupation\", \"company\", \"years_of_experience\"],\n",
    "                    \"additionalProperties\": False\n",
    "                },\n",
    "                \"location\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"city\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"City name\"\n",
    "                        },\n",
    "                        \"country\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Country name\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"city\", \"country\"],\n",
    "                    \"additionalProperties\": False\n",
    "                },\n",
    "                \"interests\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\n",
    "                        \"type\": \"string\"\n",
    "                    },\n",
    "                    \"description\": \"List of hobbies or interests mentioned\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"personal_info\", \"professional_info\", \"location\", \"interests\"],\n",
    "            \"additionalProperties\": False\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Sample text to extract from\n",
    "sample_text = \"\"\"\n",
    "Meet Dr. Maria Rodriguez, a 23-year-old data scientist at Joe AI Labs \n",
    "with 8 years of experience in machine learning. She lives in Barcelona, Spain, \n",
    "and enjoys hiking, photography, and reading science fiction novels. \n",
    "You can reach her at maria.r@joe.ai for collaboration opportunities.\n",
    "\"\"\"\n",
    "\n",
    "structured_payload = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"developer\",\n",
    "            \"content\": \"You extract structured information from text and return it in the specified JSON format.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Extract all relevant information from this text:\\n\\n{sample_text}\"\n",
    "        }\n",
    "    ],\n",
    "    \"response_format\": user_schema,\n",
    "    \"max_completion_tokens\": 2000,\n",
    "    \"reasoning_effort\": \"low\"\n",
    "}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Structured Output Demo with Strict JSON Schema\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nInput text:\\n{sample_text}\")\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(\"Schema enforces:\")\n",
    "print(\"  - Exact field names and types\")\n",
    "print(\"  - Required fields (no missing data)\")\n",
    "print(\"  - No additional properties\")\n",
    "print(\"  - Nested object structure\")\n",
    "print(f\"{'=' * 70}\\n\")\n",
    "\n",
    "response = requests.post(url, headers=headers, json=structured_payload)\n",
    "result = response.json()\n",
    "response_content = result['choices'][0]['message']['content']\n",
    "\n",
    "print(\"Raw response:\")\n",
    "print(response_content)\n",
    "\n",
    "# Parse and validate\n",
    "try:\n",
    "    parsed_data = json.loads(response_content)\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(\" Response successfully parsed as JSON\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "    print(\"\\nFormatted extraction:\")\n",
    "    print(json.dumps(parsed_data, indent=2))\n",
    "    \n",
    "    # Demonstrate accessing nested fields\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(\"Accessing structured data:\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "    print(f\"  Name: {parsed_data['personal_info']['full_name']}\")\n",
    "    print(f\"  Age: {parsed_data['personal_info']['age']}\")\n",
    "    print(f\"  Job: {parsed_data['professional_info']['occupation']}\")\n",
    "    print(f\"  Company: {parsed_data['professional_info']['company']}\")\n",
    "    print(f\"  Location: {parsed_data['location']['city']}, {parsed_data['location']['country']}\")\n",
    "    print(f\"  Experience: {parsed_data['professional_info']['years_of_experience']} years\")\n",
    "    print(f\"  Interests: {', '.join(parsed_data['interests'])}\")\n",
    "    \n",
    "    print(f\"\\n All required fields present and correctly typed!\")\n",
    "    \n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"\\n JSON parsing error: {e}\")\n",
    "except KeyError as e:\n",
    "    print(f\"\\n Missing expected field: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e17c7aa35adf28c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Streaming Responses\n",
    "\n",
    "### Theory: Server-Sent Events\n",
    "\n",
    "**How It Works:**\n",
    "1. Set `stream: true`\n",
    "2. Receive chunks as `data: {...}\\n\\n`\n",
    "3. Accumulate `delta.content`\n",
    "4. Stop at `data: [DONE]`\n",
    "\n",
    "**Benefits:** Lower perceived latency, progressive UI updates\n",
    "\n",
    "**Trade-off:** More complex error handling, can't validate before displaying\n",
    "\n",
    "---\n",
    "\n",
    "### Demo 8.1: Stream Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9c65b152ea1a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Build streaming request URL and headers\n",
    "endpoint = os.environ['DIAL_API_ENDPOINT']\n",
    "deployment = os.environ['DIAL_DEPLOYMENT']\n",
    "api_version = os.environ['DIAL_API_VERSION']\n",
    "api_key = os.environ['DIAL_API_KEY']\n",
    "\n",
    "url = f\"{endpoint}/openai/deployments/{deployment}/chat/completions?api-version={api_version}\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"api-key\": api_key\n",
    "}\n",
    "\n",
    "# Streaming request payload\n",
    "streaming_payload = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"developer\",\n",
    "            \"content\": \"You are a technical writer who explains concepts clearly. Respond directly without extensive reasoning.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain how HTTP requests work, step by step.\"\n",
    "        }\n",
    "    ],\n",
    "    \"max_completion_tokens\": 3000,\n",
    "    \"reasoning_effort\": \"minimal\",  # Add for GPT-5\n",
    "    \"stream\": True  # Enable streaming\n",
    "}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Streaming Demo\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nStreaming response (tokens appear in real-time):\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "full_content = \"\"\n",
    "chunk_count = 0\n",
    "\n",
    "try:\n",
    "    # Make streaming request\n",
    "    response = requests.post(url, headers=headers, json=streaming_payload, stream=True, timeout=60)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    # Process Server-Sent Events\n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            line_str = line.decode('utf-8')\n",
    "            \n",
    "            # SSE format: \"data: {...}\"\n",
    "            if line_str.startswith('data: '):\n",
    "                data_str = line_str[6:]  # Remove \"data: \" prefix\n",
    "                \n",
    "                # Check for stream end\n",
    "                if data_str.strip() == '[DONE]':\n",
    "                    break\n",
    "                \n",
    "                try:\n",
    "                    # Parse JSON chunk\n",
    "                    chunk = json.loads(data_str)\n",
    "                    chunk_count += 1\n",
    "                    \n",
    "                    # Extract delta content\n",
    "                    if 'choices' in chunk and len(chunk['choices']) > 0:\n",
    "                        delta = chunk['choices'][0].get('delta', {})\n",
    "                        content = delta.get('content', '')\n",
    "                        \n",
    "                        if content:\n",
    "                            full_content += content\n",
    "                            # Print token(s) as they arrive\n",
    "                            print(content, end='', flush=True)\n",
    "                        \n",
    "                        # Check for finish reason\n",
    "                        finish_reason = chunk['choices'][0].get('finish_reason')\n",
    "                        if finish_reason:\n",
    "                            print(f\"\\n\\n[Stream ended: {finish_reason}]\")\n",
    "                \n",
    "                except json.JSONDecodeError:\n",
    "                    pass  # Skip malformed JSON\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n\\n{'=' * 70}\")\n",
    "    print(f\"Streaming Statistics:\")\n",
    "    print(f\"  Total chunks received: {chunk_count}\")\n",
    "    print(f\"  Total characters: {len(full_content)}\")\n",
    "    print(f\"  Time elapsed: {elapsed:.2f}s\")\n",
    "    if chunk_count > 0:\n",
    "        print(f\"  Avg time per chunk: {(elapsed/chunk_count)*1000:.1f}ms\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"\\n\\nStreaming error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4634d66bd48cb49c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 9: Vision (Multimodal)\n",
    "\n",
    "### Theory: Image Analysis\n",
    "\n",
    "**Capabilities:**\n",
    "- GPT-4o: text + images\n",
    "- GPT-5: text only (for now)\n",
    "\n",
    "**Detail Levels:**\n",
    "- `low`: 85 tokens flat (classification)\n",
    "- `high`: variable tokens, tiled processing (detailed analysis)\n",
    "\n",
    "**Formats:** Base64 data or public URLs. Max 20MB/image, 50 images/request.\n",
    "\n",
    "---\n",
    "\n",
    "### Demo 9.1: Analyze Vacation Photo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbef5fcc296b6dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "# Use GPT-4o for vision support\n",
    "gpt4o_deployment = os.environ.get('DIAL_GPT4O_DEPLOYMENT', 'gpt-4o-mini-2024-07-18')\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Vision Demo: Analyzing Vacation Photos\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Use images from local images/ folder\n",
    "IMAGE_1 = \"images/background-vacation1.jpeg\"\n",
    "IMAGE_2 = \"images/background-vacation2.jpeg\"\n",
    "\n",
    "def load_image_base64(image_path: str):\n",
    "    \"\"\"Load image and convert to base64.\"\"\"\n",
    "    path = Path(image_path)\n",
    "    \n",
    "    # If relative path, resolve from current directory\n",
    "    if not path.is_absolute():\n",
    "        path = Path.cwd() / path\n",
    "    \n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Image not found: {path}\")\n",
    "    \n",
    "    with path.open(\"rb\") as image_file:\n",
    "        image_bytes = image_file.read()\n",
    "    \n",
    "    return base64.b64encode(image_bytes).decode(\"utf-8\"), path\n",
    "\n",
    "# Load first vacation image\n",
    "image_data, image_file = load_image_base64(IMAGE_1)\n",
    "print(f\"\\nAnalyzing: {image_file.name}\")\n",
    "\n",
    "# Build payload with base64-encoded image\n",
    "payload = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Describe this vacation photo. What location might this be? What activities or experiences does it suggest?\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{image_data}\",\n",
    "                        \"detail\": \"high\"  # Use high detail for better analysis\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"max_tokens\": 400,\n",
    "    \"temperature\": 0.7\n",
    "}\n",
    "\n",
    "# Build GPT-4o endpoint\n",
    "endpoint = os.environ['DIAL_API_ENDPOINT']\n",
    "api_version = os.environ['DIAL_API_VERSION']\n",
    "gpt4o_url = f\"{endpoint}/openai/deployments/{gpt4o_deployment}/chat/completions?api-version={api_version}\"\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"api-key\": os.environ['DIAL_API_KEY']\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = requests.post(gpt4o_url, headers=headers, json=payload, timeout=60)\n",
    "    response.raise_for_status()\n",
    "    result = response.json()\n",
    "\n",
    "    print(\"\\nGPT-4o Analysis:\")\n",
    "    print(\"-\" * 70)\n",
    "    print(result['choices'][0]['message']['content'])\n",
    "\n",
    "    usage = result.get('usage', {})\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"Token Usage:\")\n",
    "    print(f\"  Input tokens: {usage.get('prompt_tokens', 0)}\")\n",
    "    print(f\"  Output tokens: {usage.get('completion_tokens', 0)}\")\n",
    "    print(f\"  Total: {usage.get('total_tokens', 0)}\")\n",
    "    print(\"\\n High detail image analysis uses more tokens but provides richer descriptions\")\n",
    "\n",
    "    # Optional: Analyze second image\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"Analyzing second vacation photo...\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    image_data2, image_file_2 = load_image_base64(IMAGE_2)\n",
    "\n",
    "    payload2 = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": \"Describe this vacation scene. Compare the mood and setting to a beach destination.\"},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data2}\", \"detail\": \"low\"}}\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 300,\n",
    "        \"temperature\": 0.7\n",
    "    }\n",
    "\n",
    "    response2 = requests.post(gpt4o_url, headers=headers, json=payload2, timeout=60)\n",
    "    result2 = response2.json()\n",
    "\n",
    "    print(f\"\\nAnalyzing: {image_file_2.name}\")\n",
    "    print(\"-\" * 70)\n",
    "    print(result2['choices'][0]['message']['content'])\n",
    "\n",
    "    usage2 = result2.get('usage', {})\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"Token Comparison (detail='low' vs detail='high'):\")\n",
    "    print(f\"  Image 1 (high detail): {usage.get('prompt_tokens', 0)} input tokens\")\n",
    "    print(f\"  Image 2 (low detail):  {usage2.get('prompt_tokens', 0)} input tokens\")\n",
    "    print(f\"  Savings: ~{usage.get('prompt_tokens', 0) - usage2.get('prompt_tokens', 0)} tokens with 'low' detail\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n Error: {e}\")\n",
    "    if hasattr(e, 'response') and e.response is not None:\n",
    "        try:\n",
    "            error_detail = e.response.json()\n",
    "            print(f\"Details: {error_detail}\")\n",
    "        except:\n",
    "            print(f\"Response: {e.response.text[:500]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Key Takeaways:\")\n",
    "print(\"=\" * 70)\n",
    "print(\" GPT-4o supports vision; GPT-5 is text-only\")\n",
    "print(\" 'detail': 'high' -> better analysis, more tokens\")\n",
    "print(\" 'detail': 'low' -> faster, cheaper, good for classification\")\n",
    "print(\" Images can be base64-encoded or public URLs\")\n",
    "print(\" Remove images from history after analysis to save tokens\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87afeb7595b73fb9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 10: Production Readiness & Operational Guardrails\n",
    "\n",
    "### Theory: Production Readiness\n",
    "\n",
    "**Always Check `finish_reason`:**\n",
    "- `stop`: Normal completion\n",
    "- `length`: Truncated (increase max_completion_tokens)\n",
    "- `content_filter`: Blocked by safety filters\n",
    "- `tool_calls`: Model wants to call a function\n",
    "\n",
    "**Rate Limits:**\n",
    "- HTTP 429: Too many requests\n",
    "- Headers: `x-ratelimit-remaining-requests`, `x-ratelimit-remaining-tokens`\n",
    "- Implement exponential backoff: 1s, 2s, 4s, 8s...\n",
    "\n",
    "**Usage Monitoring:**\n",
    "- Track token usage per request\n",
    "- Monitor cumulative costs\n",
    "- Log response times and errors\n",
    "- Use request IDs for debugging\n",
    "\n",
    "**Model Selection Guide:**\n",
    "- **GPT-5**: Complex reasoning, math, code analysis (slowest, highest cost)\n",
    "- **GPT-5-mini**: Balanced reasoning tasks, good for most use cases (moderate speed & cost)\n",
    "- **GPT-5-nano**: Lightweight reasoning for edge/mobile deployments (faster, lower cost)\n",
    "- **GPT-4o**: General tasks, speed-critical apps (fast, cost-effective)\n",
    "- **GPT-4o-mini**: Simple classification, high-volume tasks (fastest, cheapest)\n",
    "\n",
    "---\n",
    "\n",
    "### Demo 10.1: Detecting Truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e0cdef467644e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Build API request components\n",
    "endpoint = os.environ['DIAL_API_ENDPOINT']\n",
    "deployment = os.environ['DIAL_DEPLOYMENT']\n",
    "api_version = os.environ['DIAL_API_VERSION']\n",
    "api_key = os.environ['DIAL_API_KEY']\n",
    "\n",
    "url = f\"{endpoint}/openai/deployments/{deployment}/chat/completions?api-version={api_version}\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"api-key\": api_key\n",
    "}\n",
    "\n",
    "truncated_payload = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"developer\", \"content\": \"You are a technical writer.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain microservices architecture in detail with examples.\"}\n",
    "    ],\n",
    "    \"max_completion_tokens\": 30,  # Intentionally too small!\n",
    "    \"reasoning_effort\": \"minimal\"\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=truncated_payload, timeout=30)\n",
    "result = response.json()\n",
    "choice = result['choices'][0]\n",
    "content = choice['message'].get('content', '')\n",
    "\n",
    "# Show the truncated response\n",
    "if content:\n",
    "    preview = content[:150] if len(content) > 150 else content\n",
    "    print(f\"Response (truncated):\\n{preview}...\")\n",
    "else:\n",
    "    # GPT-5 might use all tokens for reasoning\n",
    "    reasoning_tokens = result['usage'].get('completion_tokens_details', {}).get('reasoning_tokens', 0)\n",
    "    print(f\"Response: (empty - {reasoning_tokens} reasoning tokens used, no visible output)\")\n",
    "\n",
    "print(f\"\\nFinish reason: {choice['finish_reason']}\")\n",
    "\n",
    "if choice['finish_reason'] == 'length':\n",
    "    print(\"\\nTRUNCATED! The response was cut off mid-sentence.\")\n",
    "    print(f\"   Token usage: {result['usage']['prompt_tokens']} prompt + {result['usage']['completion_tokens']} completion = {result['usage']['total_tokens']} total\")\n",
    "    \n",
    "    # Show reasoning token breakdown for GPT-5\n",
    "    if 'completion_tokens_details' in result['usage']:\n",
    "        reasoning = result['usage']['completion_tokens_details'].get('reasoning_tokens', 0)\n",
    "        output = result['usage']['completion_tokens'] - reasoning\n",
    "        if reasoning > 0:\n",
    "            print(f\"   Breakdown: {reasoning} reasoning tokens + {output} output tokens\")\n",
    "    \n",
    "    print(\"\\nFix: Increase max_completion_tokens to 200-300 for full response\")\n",
    "else:\n",
    "    print(\"Response completed naturally (finish_reason: stop)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d7f0ac",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Demo 10.2: Exponential Backoff Pattern\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5473d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Build API request components\n",
    "endpoint = os.environ['DIAL_API_ENDPOINT']\n",
    "deployment = os.environ['DIAL_DEPLOYMENT']\n",
    "api_version = os.environ['DIAL_API_VERSION']\n",
    "api_key = os.environ['DIAL_API_KEY']\n",
    "\n",
    "url = f\"{endpoint}/openai/deployments/{deployment}/chat/completions?api-version={api_version}\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"api-key\": api_key\n",
    "}\n",
    "\n",
    "def call_with_retry(url, headers, payload, max_retries=3):\n",
    "    '''Call API with exponential backoff on rate limits.'''\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.post(url, headers=headers, json=payload, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            if e.response.status_code == 429 and attempt < max_retries - 1:\n",
    "                wait_time = 2 ** attempt  # 1s, 2s, 4s\n",
    "                print(f\"Rate limited! Waiting {wait_time}s before retry {attempt + 2}/{max_retries}...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                raise\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            raise\n",
    "    \n",
    "print(\"Retry pattern: 1s -> 2s -> 4s -> 8s...\")\n",
    "print(\"Always implement exponential backoff for production!\")\n",
    "print(\"\\nExample: If you hit rate limit (429), the function will:\")\n",
    "print(\"  1st retry: wait 1s\")\n",
    "print(\"  2nd retry: wait 2s\") \n",
    "print(\"  3rd retry: wait 4s\")\n",
    "print(\"  Then fail if still rate limited\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c87a7d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Demo 10.3: Track Usage and Costs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e198734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Build API request components\n",
    "endpoint = os.environ['DIAL_API_ENDPOINT']\n",
    "deployment = os.environ['DIAL_DEPLOYMENT']\n",
    "api_version = os.environ['DIAL_API_VERSION']\n",
    "api_key = os.environ['DIAL_API_KEY']\n",
    "\n",
    "url = f\"{endpoint}/openai/deployments/{deployment}/chat/completions?api-version={api_version}\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"api-key\": api_key\n",
    "}\n",
    "\n",
    "# Track usage across multiple requests\n",
    "total_tokens = 0\n",
    "total_cost = 0.0\n",
    "\n",
    "for i in range(3):\n",
    "    test_payload = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": f\"Give me a random fact about space #{i+1}\"}\n",
    "        ],\n",
    "        \"max_completion_tokens\": 100,\n",
    "        \"reasoning_effort\": \"minimal\"\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, headers=headers, json=test_payload, timeout=30)\n",
    "    result = response.json()\n",
    "    \n",
    "    # Calculate usage\n",
    "    prompt_tokens = result['usage']['prompt_tokens']\n",
    "    completion_tokens = result['usage']['completion_tokens']\n",
    "    request_tokens = result['usage']['total_tokens']\n",
    "    \n",
    "    # GPT-5-mini pricing: $0.25/1M input, $2.00/1M output\n",
    "    request_cost = (prompt_tokens * 0.00000025) + (completion_tokens * 0.000002)\n",
    "    \n",
    "    total_tokens += request_tokens\n",
    "    total_cost += request_cost\n",
    "    \n",
    "    print(f\"Request {i+1}: {request_tokens} tokens, ${request_cost:.6f}\")\n",
    "\n",
    "print(f\"\\nTotal usage: {total_tokens} tokens\")\n",
    "print(f\"Total cost: ${total_cost:.6f}\")\n",
    "print(f\"Average per request: {total_tokens/3:.1f} tokens, ${total_cost/3:.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
